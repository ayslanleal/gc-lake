# Databricks notebook source
# MAGIC %sql
# MAGIC 
# MAGIC select * from bronze_gc.tb_lobby_stats_player

# COMMAND ----------

import sys 

sys.path.insert(0, '../lib')

from utils import db
from delta.tables import *
from pyspark.sql import functions as F
from pyspark.sql import window

# COMMAND ----------

tb_origin = 'bronze_gc.tb_lobby_stats_player'
tb_target = 'silver_gc.tb_lobby_stats_player'

id_origin = 'idLobbyGame','idPlayer'
id_target = ['idLobbyGame','idPlayer']

strongly_date_origin = 'dtCreatedAt'
strongly_date_target = 'dtCreatedAt'

checkpoint_path = f'/mnt/datalake/silver/{tb_target.split(".")[-1]}_checkpoint'



# COMMAND ----------

query = db.import_query('queries/tb_lobby_stats_player.sql')
if not db.table_exists(tb_target, spark):
    query = query.replace(" Op,","")
    df = db.etl(query,tb_origin, spark)
    df.coalesce(1).write.mode('overwrite').format('delta').saveAsTable(tb_target)
    

# COMMAND ----------

query = db.import_query('queries/tb_lobby_stats_player.sql')
delta_table = DeltaTable.forName(spark, tb_target)

def upsert_delta(df, batchId, query, delta_table, id_field, strongly_date):
    
    join = " and ".join([f'd.{i} = c.{i}' for i in id_field])
    
    w = window.Window().partitionBy(id_field).orderBy(F.desc(strongly_date))
    
    df = (df.withColumn('Op', F.when(df._change_type == "insert", "I")
                               .when(df._change_type == "update_preimage", "U")
                               .when(df._change_type == "update_postimage", "U")
                               .when(df._change_type == "delete", "D"))
                               .withColumn('rn', F.row_number().over(w))
                               .filter("rn=1")
                               .drop(F.col('rn')))
    
    view_name = f"{tb_target.split('.')[-1]}_view"
    df.createOrReplaceGlobalTempView(view_name)
    

    view_name = f"global_temp.{tb_target.split('.')[-1]}_view"
    cdc_data = db.etl(query, view_name, spark)
    
    (delta_table.alias("d")
                .merge(cdc_data.alias("c"), join) 
                .whenMatchedDelete(condition = "c.Op = 'D'")
                .whenMatchedUpdateAll(condition = "c.Op ='U'")
                .whenNotMatchedInsertAll(condition = "c.Op = 'I'")
                .execute())
    return None


df_stream = spark.readStream.format("delta").option("readChangeFeed", "true").option("startingVersion", 0).table(tb_origin)

stream = (df_stream.writeStream.format('delta')
                   .foreachBatch( lambda df, batchId: upsert_delta(df, batchId, query, delta_table, id_target, strongly_date_target))
                   .option('checkpointLocation', checkpoint_path)
                   .start())



# COMMAND ----------


